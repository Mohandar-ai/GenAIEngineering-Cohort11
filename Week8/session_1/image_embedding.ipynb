{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Embeddings Tutorial with Hugging Face\n",
    "=================================================\n",
    "  \n",
    "[View on Google Colab](https://colab.research.google.com/drive/1AQPc0q6kN_ADS_Fbu-FIE_YrnQXb-G0o?usp=sharing)\n",
    "  \n",
    "In this notebook you will learn:  \n",
    "1. Loading MNIST dataset from Hugging Face  \n",
    "2. Creating embeddings using Vision Transformer  \n",
    "3. Visualizing embeddings in 2D space  \n",
    "4. Computing and displaying similarity matrices  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy torch transformers datasets pillow matplotlib seaborn librosa scikit-learn\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoModel, AutoImageProcessor, AutoModelForImageClassification\n",
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "from typing import List, Union, Optional\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the MNIST Dataset from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist_dataset(num_samples: int = 1000) -> tuple:\n",
    "    \"\"\"\n",
    "    Load MNIST dataset from Hugging Face datasets.\n",
    "    \n",
    "    Args:\n",
    "        num_samples (int, optional): Number of samples to load from the dataset.\n",
    "            Default is 1000 for faster processing.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (images, labels) where images is a list of PIL Images and \n",
    "               labels is a list of corresponding digit labels.\n",
    "    \"\"\"\n",
    "    print(f\"Loading MNIST dataset from Hugging Face...\")\n",
    "    \n",
    "    # Load the MNIST dataset from Hugging Face\n",
    "    dataset = load_dataset(\"mnist\", split=\"train\")\n",
    "    \n",
    "    # Take a subset for demonstration\n",
    "    dataset = dataset.select(range(min(num_samples, len(dataset))))\n",
    "    \n",
    "    # Extract images and labels\n",
    "    images = dataset[\"image\"]\n",
    "    labels = dataset[\"label\"]\n",
    "    \n",
    "    print(f\"Loaded {len(images)} images with labels\")\n",
    "    print(f\"Image format: {images[0].mode}, Size: {images[0].size}\")\n",
    "    \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load MNIST dataset\n",
    "print(\"\\n1. Loading MNIST Dataset:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "images, labels = load_mnist_dataset(num_samples=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_sample_images(images: List[Image.Image], labels: List[int], num_samples: int = 20):\n",
    "    \"\"\"\n",
    "    Display a grid of sample images with their corresponding labels and embedding info.\n",
    "    \n",
    "    Args:\n",
    "        images (List[Image.Image]): List of PIL images.\n",
    "        labels (List[int]): List of corresponding labels.\n",
    "        num_samples (int, optional): Number of sample images to display. Default is 20.\n",
    "    \"\"\"\n",
    "    # Select random samples\n",
    "    indices = np.random.choice(len(images), min(num_samples, len(images)), replace=False)\n",
    "    \n",
    "    # Create subplot grid\n",
    "    cols = 5\n",
    "    rows = (num_samples + cols - 1) // cols\n",
    "    \n",
    "    plt.figure(figsize=(15, 3 * rows))\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        plt.subplot(rows, cols, i + 1)\n",
    "        \n",
    "        # Display the image\n",
    "        plt.imshow(images[idx], cmap='gray')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Add title with label \n",
    "        plt.title(f'Label: {labels[idx]}', \n",
    "                 fontsize=10)\n",
    "    \n",
    "    plt.suptitle('Sample MNIST Images', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Display sample images\n",
    "print(\"\\n2. Sample Images from Dataset:\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "visualize_sample_images(images, labels, num_samples=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the Image Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_image_embeddings(\n",
    "    images: Union[Image.Image, List[Image.Image]], \n",
    "    model_name: str = \"google/vit-base-patch16-224\",\n",
    "    normalize: bool = True,\n",
    "    device: Optional[str] = None\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Create dense vector embeddings for images using pre-trained vision transformer models.\n",
    "    \n",
    "    This function uses Hugging Face vision models to convert images into numerical\n",
    "    representations that capture visual features. These embeddings can be used\n",
    "    for image similarity search, clustering, classification, and retrieval tasks.\n",
    "    \n",
    "    Args:\n",
    "        images (Union[Image.Image, List[Image.Image]]): Single PIL Image or list of \n",
    "            PIL Images to embed. Each image will be converted to a fixed-size vector.\n",
    "        model_name (str, optional): Name of the pre-trained vision model from Hugging Face.\n",
    "            Default is \"google/vit-base-patch16-224\" (Vision Transformer). Other options:\n",
    "            - \"microsoft/resnet-50\" (ResNet-50 CNN architecture)\n",
    "            - \"facebook/deit-base-distilled-patch16-224\" (Distilled Vision Transformer)\n",
    "            - \"microsoft/swin-base-patch4-window7-224\" (Swin Transformer)\n",
    "        normalize (bool, optional): Whether to normalize embeddings to unit vectors.\n",
    "            Normalized embeddings work better for cosine similarity. Default is True.\n",
    "        device (Optional[str], optional): Device to run the model on ('cpu', 'cuda').\n",
    "            If None, automatically detects available device.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Array of embeddings with shape (n_images, embedding_dim).\n",
    "            Each row represents the embedding vector for one input image.\n",
    "    \n",
    "    Example:\n",
    "        >>> # Single image embedding\n",
    "        >>> image = Image.open(\"path/to/image.jpg\")\n",
    "        >>> embedding = create_image_embeddings(image)\n",
    "        >>> print(f\"Embedding shape: {embedding.shape}\")\n",
    "        \n",
    "        >>> # Multiple images\n",
    "        >>> images = [image1, image2, image3]\n",
    "        >>> embeddings = create_image_embeddings(images)\n",
    "        >>> print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Handle input format - convert single input to list\n",
    "    if isinstance(images, Image.Image):\n",
    "        images = [images]\n",
    "        single_input = True\n",
    "    else:\n",
    "        single_input = False\n",
    "    \n",
    "    # Step 2: Determine compute device (GPU if available, else CPU)\n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    print(f\"Using device: {device}\")\n",
    "    print(f\"Loading vision model: {model_name}\")\n",
    "    \n",
    "    # Step 3: Load pre-trained image processor and model\n",
    "    # The image processor handles image preprocessing (resize, normalize, etc.)\n",
    "    try:\n",
    "        image_processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "        # Try to load as a feature extraction model first\n",
    "        model = AutoModel.from_pretrained(model_name)\n",
    "    except:\n",
    "        # Fallback to classification model if feature extraction model not available\n",
    "        print(\"Feature extraction model not available, using classification model...\")\n",
    "        from transformers import AutoFeatureExtractor\n",
    "        try:\n",
    "            image_processor = AutoFeatureExtractor.from_pretrained(model_name)\n",
    "        except:\n",
    "            image_processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "        model = AutoModelForImageClassification.from_pretrained(model_name)\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    \n",
    "    # Step 4: Convert MNIST grayscale images to RGB for compatibility\n",
    "    print(f\"Processing {len(images)} image(s)...\")\n",
    "    processed_images = []\n",
    "    \n",
    "    for i, image in enumerate(images):\n",
    "        # Convert grayscale to RGB if needed (MNIST images are grayscale)\n",
    "        if image.mode != 'RGB':\n",
    "            # Convert grayscale to RGB by duplicating the single channel\n",
    "            image = image.convert('RGB')\n",
    "        processed_images.append(image)\n",
    "    \n",
    "    # Step 5: Apply image preprocessing (resize, normalize, tensor conversion)\n",
    "    print(\"Applying image preprocessing...\")\n",
    "    inputs = image_processor(processed_images, return_tensors=\"pt\")\n",
    "    \n",
    "    # Move processed inputs to the same device as the model\n",
    "    pixel_values = inputs['pixel_values'].to(device)\n",
    "    \n",
    "    # Step 6: Generate embeddings using the vision model\n",
    "    print(\"Generating image embeddings...\")\n",
    "    with torch.no_grad():  # Disable gradient computation for efficiency\n",
    "        # Forward pass through the vision model\n",
    "        outputs = model(pixel_values=pixel_values)\n",
    "        \n",
    "        # Extract embeddings based on model architecture\n",
    "        if hasattr(outputs, 'last_hidden_state'):\n",
    "            # For models like ViT that return sequence outputs\n",
    "            # Use the [CLS] token embedding (first token) for image representation\n",
    "            embeddings = outputs.last_hidden_state[:, 0, :]  # Shape: (batch_size, hidden_size)\n",
    "        elif hasattr(outputs, 'pooler_output'):\n",
    "            # For models with pooler output\n",
    "            embeddings = outputs.pooler_output\n",
    "        elif hasattr(outputs, 'logits'):\n",
    "            # For classification models, extract features before final layer\n",
    "            # We need to access the hidden features, not the classification logits\n",
    "            if hasattr(model, 'vit'):\n",
    "                # For ViT models wrapped in classification head\n",
    "                embeddings = model.vit(pixel_values=pixel_values).last_hidden_state[:, 0, :]\n",
    "            elif hasattr(model, 'resnet'):\n",
    "                # For ResNet models\n",
    "                embeddings = model.resnet(pixel_values=pixel_values).pooler_output\n",
    "            else:\n",
    "                # Fallback: use logits as embeddings (not ideal but functional)\n",
    "                print(\"Warning: Using classification logits as embeddings\")\n",
    "                embeddings = outputs.logits\n",
    "        else:\n",
    "            raise ValueError(\"Could not extract embeddings from model outputs\")\n",
    "    \n",
    "    # Step 7: Move embeddings back to CPU and convert to numpy\n",
    "    embeddings = embeddings.cpu().numpy()\n",
    "    \n",
    "    # Step 8: Optional normalization for better similarity computation\n",
    "    if normalize:\n",
    "        # L2 normalization: each embedding becomes a unit vector\n",
    "        norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "        embeddings = embeddings / (norms + 1e-9)  # Avoid division by zero\n",
    "    \n",
    "    print(f\"Generated image embeddings with shape: {embeddings.shape}\")\n",
    "    \n",
    "    # Step 9: Return single embedding if single input was provided\n",
    "    if single_input:\n",
    "        return embeddings[0]\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Create embeddings\n",
    "print(\"\\n3. Creating Image Embeddings:\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "# Use a smaller subset for faster processing\n",
    "subset_size = 500\n",
    "subset_images = images[:subset_size]\n",
    "subset_labels = labels[:subset_size]\n",
    "\n",
    "embeddings = create_image_embeddings(subset_images)\n",
    "\n",
    "print(f\"Created embeddings for {len(subset_images)} images\")\n",
    "print(f\"Embedding dimension: {embeddings.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the PCA and t-SNE Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_embeddings_2d(embeddings: np.ndarray, labels: List[int], \n",
    "                           method: str = \"tsne\", title: str = \"Image Embeddings\"):\n",
    "    \"\"\"\n",
    "    Visualize high-dimensional embeddings in 2D space using dimensionality reduction.\n",
    "    \n",
    "    Args:\n",
    "        embeddings (np.ndarray): High-dimensional embeddings with shape (n_samples, embedding_dim).\n",
    "        labels (List[int]): List of labels corresponding to each embedding.\n",
    "        method (str, optional): Dimensionality reduction method ('pca' or 'tsne'). Default is 'tsne'.\n",
    "        title (str, optional): Title for the plot.\n",
    "    \"\"\"\n",
    "    print(f\"Reducing dimensionality using {method.upper()}...\")\n",
    "    \n",
    "    if method.lower() == \"pca\":\n",
    "        # Principal Component Analysis - linear dimensionality reduction\n",
    "        reducer = PCA(n_components=2, random_state=42)\n",
    "        embeddings_2d = reducer.fit_transform(embeddings)\n",
    "        explained_variance = reducer.explained_variance_ratio_\n",
    "        print(f\"PCA explained variance ratio: {explained_variance}\")\n",
    "    elif method.lower() == \"tsne\":\n",
    "        # t-SNE - non-linear dimensionality reduction, better for visualization\n",
    "        reducer = TSNE(n_components=2, random_state=42, perplexity=30, max_iter=1000)\n",
    "        embeddings_2d = reducer.fit_transform(embeddings)\n",
    "    else:\n",
    "        raise ValueError(\"Method must be 'pca' or 'tsne'\")\n",
    "    \n",
    "    # Create the visualization\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Create a color map for the 10 digits\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, 10))\n",
    "    \n",
    "    # Plot each digit with a different color\n",
    "    for digit in range(10):\n",
    "        mask = np.array(labels) == digit\n",
    "        if np.any(mask):\n",
    "            plt.scatter(embeddings_2d[mask, 0], embeddings_2d[mask, 1], \n",
    "                       c=[colors[digit]], label=f'Digit {digit}', alpha=0.7, s=20)\n",
    "    \n",
    "    plt.title(f'{title} - {method.upper()} Visualization')\n",
    "    plt.xlabel(f'{method.upper()} Component 1')\n",
    "    plt.ylabel(f'{method.upper()} Component 2')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return embeddings_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Visualize embeddings in 2D\n",
    "print(\"\\n4. Visualizing Embeddings in 2D Space:\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "# PCA visualization\n",
    "print(\"Creating PCA visualization...\")\n",
    "pca_embeddings = visualize_embeddings_2d(embeddings, subset_labels, \n",
    "                                        method=\"pca\", \n",
    "                                        title=\"MNIST Embeddings\")\n",
    "\n",
    "# t-SNE visualization\n",
    "print(\"Creating t-SNE visualization...\")\n",
    "tsne_embeddings = visualize_embeddings_2d(embeddings, subset_labels, \n",
    "                                        method=\"tsne\", \n",
    "                                        title=\"MNIST Embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the Similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity_matrix(embeddings: np.ndarray, labels: List[int], \n",
    "                            max_samples: int = 100) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute and visualize similarity matrix for a subset of embeddings.\n",
    "    \n",
    "    Args:\n",
    "        embeddings (np.ndarray): Array of embeddings.\n",
    "        labels (List[int]): List of corresponding labels.\n",
    "        max_samples (int, optional): Maximum number of samples to include in similarity matrix.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Similarity matrix.\n",
    "    \"\"\"\n",
    "    # Take a subset for visualization\n",
    "    if len(embeddings) > max_samples:\n",
    "        indices = np.random.choice(len(embeddings), max_samples, replace=False)\n",
    "        subset_embeddings = embeddings[indices]\n",
    "        subset_labels = [labels[i] for i in indices]\n",
    "    else:\n",
    "        subset_embeddings = embeddings\n",
    "        subset_labels = labels\n",
    "    \n",
    "    # Compute cosine similarity matrix\n",
    "    similarity_matrix = np.dot(subset_embeddings, subset_embeddings.T)\n",
    "    \n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Sort by labels for better visualization\n",
    "    sorted_indices = np.argsort(subset_labels)\n",
    "    sorted_similarity = similarity_matrix[sorted_indices][:, sorted_indices]\n",
    "    sorted_labels = [subset_labels[i] for i in sorted_indices]\n",
    "    \n",
    "    # Create heatmap\n",
    "    sns.heatmap(sorted_similarity, \n",
    "                xticklabels=sorted_labels, \n",
    "                yticklabels=sorted_labels,\n",
    "                cmap='coolwarm', \n",
    "                center=0,\n",
    "                square=True,\n",
    "                cbar_kws={'label': 'Cosine Similarity'})\n",
    "    \n",
    "    plt.title('Cosine Similarity Matrix of Image Embeddings\\n(sorted by digit labels)')\n",
    "    plt.xlabel('Image Index (by digit)')\n",
    "    plt.ylabel('Image Index (by digit)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return similarity_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Compute similarity matrix\n",
    "print(\"\\n5. Computing Similarity Matrix:\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "similarity_matrix = compute_similarity_matrix(embeddings, subset_labels, \n",
    "                                            max_samples=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze the Embedding Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Analyze embedding quality\n",
    "print(\"\\n6. Embedding Quality Analysis:\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "# Compute average intra-class vs inter-class similarity\n",
    "intra_similarities = []\n",
    "inter_similarities = []\n",
    "\n",
    "for i in range(len(subset_labels)):\n",
    "    for j in range(i+1, len(subset_labels)):\n",
    "        similarity = np.dot(embeddings[i], embeddings[j])\n",
    "        \n",
    "        if subset_labels[i] == subset_labels[j]:\n",
    "            intra_similarities.append(similarity)\n",
    "        else:\n",
    "            inter_similarities.append(similarity)\n",
    "\n",
    "print(f\"Average intra-class similarity: {np.mean(intra_similarities):.4f}\")\n",
    "print(f\"Average inter-class similarity: {np.mean(inter_similarities):.4f}\")\n",
    "print(f\"Separation ratio: {np.mean(intra_similarities) / np.mean(inter_similarities):.4f}\")\n",
    "\n",
    "# Plot similarity distributions\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(intra_similarities, bins=50, alpha=0.7, label='Intra-class', density=True)\n",
    "plt.hist(inter_similarities, bins=50, alpha=0.7, label='Inter-class', density=True)\n",
    "plt.xlabel('Cosine Similarity')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Distribution of Intra-class vs Inter-class Similarities')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
